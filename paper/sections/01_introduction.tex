% =============================================================================
% Section 1: Introduction
% Target: ~800 words
% =============================================================================

% --- Paragraph 1: Motivation (Sustainable Transportation Context) ---

Autonomous vehicles (AVs) represent a transformative technology for achieving sustainable urban mobility. By reducing human-error-related collisions---which account for over 94\% of serious crashes according to the U.S.\ National Highway Traffic Safety Administration~\cite{nhtsa2022framework}---AVs promise substantial improvements in traffic safety, energy efficiency, and urban livability. These benefits align directly with the United Nations Sustainable Development Goals, particularly SDG~11 (Sustainable Cities and Communities) and SDG~13 (Climate Action)~\cite{un2015sdg}. Studies project that widespread AV adoption could reduce traffic fatalities by 90\%, decrease fuel consumption by 40\% through smoother driving patterns, and reclaim urban space currently dedicated to parking~\cite{fagnant2015av_benefits, greenblatt2015av_emissions, wadud2016av_energy}. However, realizing these benefits depends critically on achieving public trust and regulatory approval, both of which remain constrained by the opacity of the artificial intelligence systems that underpin autonomous driving~\cite{nordhoff2018av_acceptance, milakis2017av_ripple}.

% --- Paragraph 2: Technical Problem (Black-Box AI) ---

At the core of modern AV planning pipelines lies motion prediction: forecasting the future trajectories of surrounding vehicles, pedestrians, and cyclists. Transformer-based architectures have emerged as the dominant paradigm for this task, achieving state-of-the-art performance on major benchmarks. Models such as Motion Transformer (MTR)~\cite{shi2022mtr, shi2024mtrpp}, Wayformer~\cite{nayakanti2023wayformer}, GameFormer~\cite{huang2023gameformer}, and Scene Transformer~\cite{ngiam2022scene} leverage multi-head self-attention and cross-attention mechanisms to capture complex interactions among traffic agents and road geometry. Despite their strong quantitative performance, these models operate as \emph{black boxes}: the attention weights that encode inter-agent relationships, lane preferences, and temporal reasoning remain hidden from developers and safety engineers. This lack of interpretability creates three practical barriers. First, when a model produces an erroneous prediction---such as failing to anticipate a left-turning vehicle---there is no principled way to diagnose whether the failure stems from insufficient attention to the relevant agent, the target lane, or the traffic signal. Second, regulatory bodies increasingly demand explanations for safety-critical AI decisions, as codified in the European Union AI Act~\cite{eu2024ai_act} and NHTSA testing frameworks~\cite{nhtsa2022framework}. Third, without transparency, the general public lacks the evidence necessary to trust autonomous systems, ultimately delaying adoption and the associated sustainability benefits~\cite{koopman2017av_safety, zablocki2022xai_ad}.

% --- Paragraph 3: Existing Approaches (Gap Analysis) ---

Several lines of research have addressed AI interpretability, though significant gaps remain in the trajectory prediction domain. Post-hoc explanation methods such as LIME~\cite{ribeiro2016lime}, SHAP~\cite{lundberg2017shap}, and Grad-CAM~\cite{selvaraju2017gradcam} provide input-level attributions but do not leverage the structured internal attention mechanisms of Transformers. In natural language processing and computer vision, dedicated attention visualization tools---including BERTViz~\cite{vig2019bertviz}, Attention Flow~\cite{abnar2020attention}, and Transformer Explainability~\cite{chefer2021transformer}---have demonstrated that attention patterns encode interpretable relationships, though the debate on whether attention constitutes explanation continues~\cite{jain2019attention, wiegreffe2019attention}. Within trajectory prediction, recent work has begun to explore attention-based interpretability: VISTA~\cite{dasilva2025vista} visualizes pairwise interaction strength, and LMFormer~\cite{yadav2025lmformer} examines lane-conditioned attention maps. However, these efforts focus on isolated aspects of the attention spectrum---either agent--agent interactions or lane selection---and do not provide a unified view of \emph{where} the model attends in physical space, \emph{how} its reasoning evolves across processing layers, and \emph{which} lane structures guide its predictions.

% --- Paragraph 4: Our Approach (Solution) ---

In this paper, we present a spatial attention visualization framework for Transformer-based trajectory prediction that goes beyond depicting abstract attention matrices. We specifically adopt a Transformer architecture because its multi-head attention mechanism provides a built-in interpretability window: the attention weights directly reveal the model's spatial focus, agent and lane priorities, and layer-wise processing evolution. Unlike recurrent architectures such as LSTMs---where hidden states encode temporal dependencies in opaque, entangled vectors---or convolutional networks whose internal feature maps lack token-level semantic correspondence, Transformer attention offers explicit, per-token interaction scores that are naturally amenable to spatial visualization. Indeed, recent work by Zhou and Alecsandru~\cite{zhou2026lane_conditioning} demonstrated that lane-graph conditioning improves LSTM-based trajectory prediction on the Waymo Open Motion Dataset; however, the LSTM hidden states do not afford the spatial interpretability that Transformer attention provides, motivating our choice of architecture for interpretability-focused analysis.

Crucially, our primary contribution is a \emph{model-agnostic visualization framework} applicable to any Transformer-based trajectory predictor, not a new state-of-the-art prediction model. To demonstrate and validate this framework, we employ a lightweight variant of the MTR architecture (MTR-Lite, 8.48M parameters) as an \emph{interpretability probe}---a deliberately simplified model that enables rapid experimentation and systematic attention analysis without the computational overhead of production-scale systems. MTR-Lite is trained on 20\% of the Waymo Open Motion Dataset~\cite{ettinger2021waymo} (approximately 17,800 scenes), yielding sufficient diversity to reveal meaningful attention patterns while maintaining experimental tractability. The framework itself---encompassing attention extraction, spatial token bookkeeping, and visualization rendering---is architecture-agnostic and can be applied to any Transformer model that produces attention weights over spatially grounded tokens, including production-scale systems such as MTR++~\cite{shi2024mtrpp}, Wayformer~\cite{nayakanti2023wayformer}, and QCNet~\cite{zhou2023qcnet}. Our key technical innovation is a \emph{spatial token bookkeeping} mechanism that maintains a bidirectional mapping between discrete token indices and their physical BEV coordinates, enabling attention weights to be projected as continuous heatmaps directly onto the traffic scene. Using Gaussian splatting for agent tokens and polyline painting for lane tokens, the resulting visualizations provide a spatially grounded view of the model's attention allocation and its progressive evolution across processing layers.

Importantly, we go beyond visualization as an end in itself. By systematically analyzing the spatial distribution of attention across 100--200 Waymo validation scenes, we uncover \textbf{quantifiable safety-relevant patterns}. We find that failed predictions exhibit significantly lower attention entropy (5.72 vs.\ 5.94 bits) and elevated self-attention compared to successful predictions---a ``tunnel vision'' failure mode in which the model over-focuses on the ego agent when it should be distributing attention more broadly. This diagnostic pattern has direct implications for collision risk, as it reveals that prediction failures are accompanied by a measurable, detectable attention pathology. We further design a counterfactual analysis methodology using controllable scene editing (agent removal, injection, and traffic signal manipulation), enabling causal---rather than merely correlational---reasoning about how individual traffic elements influence model attention and prediction outcomes. This combination of spatial visualization, failure diagnostics, and causal experimentation transforms attention analysis from a qualitative illustration into a rigorous diagnostic tool.

% --- Paragraph 5: Contributions (Bullet List) ---

The main contributions of this work are as follows:
\begin{itemize}[leftmargin=*,labelsep=4.9mm]
    \item We propose a \textbf{spatial attention visualization system} that maps abstract Transformer attention weights onto bird's-eye-view traffic scenes via Gaussian splatting and polyline painting, providing the first spatially grounded interpretation of attention in trajectory prediction.
    \item We identify a \textbf{tunnel vision failure mode} in which failed predictions exhibit significantly lower attention entropy and elevated self-attention compared to successful predictions, providing a safety-critical diagnostic signal that links measurable attention pathology to prediction failure.
    \item We design a \textbf{counterfactual attention analysis methodology} using controllable scene generation, providing the infrastructure for causal---rather than merely correlational---reasoning about how individual traffic elements influence model attention and prediction outcomes.
    \item We provide \textbf{quantitative attention diagnostics}, including layer-wise entropy analysis revealing hierarchical specialization---encoder layers progressively focus on nearby agents (entropy 5.64$\to$5.36 bits) while the final layer reverses to broad map attention (entropy 5.92 bits, 63.6\% map tokens)---and a distance mask ablation demonstrating that suppressing far-range attention degrades accuracy by 4.7\%, demonstrating that distant context is valuable and naive pruning is harmful.
    \item We demonstrate the framework across \textbf{diverse driving scenarios}---dense intersections (42.3\% agent attention), sparse highways (18.4\% agent attention, mean attended distance 21.4\,m vs.\ 17.0\,m at intersections), and failure cases---revealing how the model dynamically adapts its spatial attention distribution to scene complexity.
\end{itemize}

% --- Paragraph 6: Significance (Sustainability Impact) ---

Beyond its technical contributions, this work has direct implications for sustainable transportation. The discovery that prediction failures are accompanied by a measurable tunnel vision pathology---lower entropy and elevated self-attention---has immediate practical consequences: this attention signature could serve as a runtime monitor to flag unreliable predictions before they propagate to the planning module, preventing potential collisions. By quantifying failure-associated attention patterns and analyzing scene-type adaptation, we provide actionable guidance for model developers and regulatory bodies alike. For regulators, spatially grounded attention visualizations offer the kind of human-readable evidence needed to certify AV behavior in complex traffic scenarios, particularly as the European Union AI Act~\cite{eu2024ai_act} establishes explainability requirements for high-risk AI systems. For the public, the ability to see that an autonomous vehicle ``looks at'' the correct lanes, traffic signals, and nearby agents before making predictions builds the transparency necessary for trust~\cite{atakishiyev2024xai_ad_survey, nordhoff2018av_acceptance}. Furthermore, our distance mask ablation reveals that naive attention pruning strategies---which might be pursued for computational efficiency---actually degrade prediction accuracy by 4.7\%, cautioning against premature optimization and underscoring the need for interpretability-guided model compression rather than blind sparsification. Ultimately, by combining interpretability with safety diagnostics, our framework helps remove key barriers to safe AV deployment, contributing to the broader goal of reducing road fatalities, lowering transportation emissions, and creating more walkable, livable cities~\cite{taiebat2018av_sustainability, milakis2017av_ripple}.

The remainder of this paper is organized as follows. Section~\ref{sec:related_work} reviews related work on trajectory prediction, attention visualization, and explainable AI for autonomous driving. Section~\ref{sec:method} describes the MTR-Lite architecture, attention extraction mechanism, and visualization pipeline. Section~\ref{sec:results} presents quantitative evaluation results and visualization examples. Section~\ref{sec:discussion} discusses the interpretability insights, sustainability implications, and limitations. Section~\ref{sec:conclusions} concludes with future directions.
