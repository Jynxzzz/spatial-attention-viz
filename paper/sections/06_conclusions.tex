% =============================================================================
% Section 6: Conclusions
% Target: ~500 words
% =============================================================================

This paper presented a spatial attention visualization framework for Transformer-based trajectory prediction that moves beyond abstract attention matrices to provide spatially grounded, interpretable insights into model behavior. By combining a novel spatial token bookkeeping mechanism with Gaussian splatting and polyline painting techniques, we demonstrated how attention weights can be projected as continuous heatmaps onto bird's-eye-view traffic scenes, revealing \emph{where} the model looks, \emph{how} its reasoning evolves across layers, and \emph{which} road structures guide its predictions.

Our analysis across 100--200 Waymo Open scenes uncovered four key findings with implications for autonomous driving safety and interpretability. First, layer-wise entropy analysis revealed \emph{collaborative layer specialization} rather than monotonic attention focusing: Layers~0--2 progressively narrow from 5.64 to 5.36 bits while agent attention increases from 49.7\% to 62.4\%, but Layer~3 \emph{reverses} this trend---entropy rises to 5.92 bits (the highest of all layers) and map attention jumps to 63.6\%. This indicates a two-phase reasoning strategy in which early layers identify relevant agents and the final layer aggregates broader spatial context. Second, we identified a \emph{tunnel vision} failure mode by analyzing 1{,}115 prediction targets: failed predictions (ADE~$\geq$~3.32\,m) exhibit \emph{lower} attention entropy (5.72 vs.\ 5.94 bits) and \emph{higher} self-attention (0.049 vs.\ 0.035) than successful ones, with target speed as the dominant risk factor (7.2\,m/s for failures vs.\ 0.2\,m/s for successes). This suggests that attention entropy could serve as a real-time failure diagnostic for safety monitoring. Third, distance mask ablation experiments across 750 targets demonstrated that restricting far-range attention \emph{hurts} performance at every masking level (baseline ADE 2.872\,m worsens by at least 4.7\%), confirming that distant tokens provide essential traffic flow context, road structure inference, and indirect interaction dynamics. Fourth, scene-type analysis across 200 scenes revealed that the model dynamically adapts its attention strategy: agent attention reaches 42.3\% in dense traffic but drops to 18.4\% in sparse scenes, while highway top-5 attention distance extends to 21.4\,m compared to 17.0\,m at intersections.

These findings have direct implications for sustainable and safe autonomous driving. The tunnel vision failure mode reveals that overconfident, narrowly focused attention is a measurable precursor to prediction failures, opening a pathway toward attention-entropy-based safety monitoring that could flag dangerous predictions before they propagate to planning. The distance mask ablation demonstrates that principled \emph{observe$\to$hypothesize$\to$test} diagnostic cycles---enabled by our visualization framework---can reveal non-obvious model dependencies and prevent well-intentioned but harmful architectural simplifications. The scene-type adaptation finding confirms that Transformer attention is not a static computation but a context-sensitive reasoning process, strengthening the case for interpretability as a route to trust and regulatory certification under frameworks such as the EU AI Act.

Future work will pursue four directions. First, we will extend our analysis to larger, state-of-the-art models (e.g., MTR++, SMART) to investigate whether the layer specialization patterns and tunnel vision failure mode generalize across architectures. Second, we will develop attention regularization techniques that enforce minimum attention thresholds for vulnerable road users during training, directly addressing safety blind spots in current models. Third, we will execute the counterfactual attention experiments---for which we have designed and implemented a controlled scene editing pipeline---to establish causal (rather than merely correlational) links between attention patterns and prediction outcomes. Fourth, we will investigate entropy-guided dynamic token pruning to reduce computational overhead while preserving prediction accuracy, integrating our visualization framework into closed-loop simulation environments to evaluate whether attention-aware efficiency optimization and safety monitoring improve real-time decision-making in dynamic driving scenarios.

Importantly, the visualization and diagnostic framework presented here---spatial token bookkeeping, entropy decomposition, and the \emph{observe$\to$hypothesize$\to$test} analytical cycle---is \textbf{architecture-agnostic}. While we demonstrated it on MTR-Lite as a lightweight interpretability probe, the same tools apply unchanged to any Transformer that produces attention weights over spatially grounded tokens, including production-scale systems such as Wayformer, MTR++, and QCNet. By bridging the gap between model performance and model understanding, this work contributes to the broader goal of building autonomous vehicles that are not only accurate but also transparent, safe, and trustworthy---essential prerequisites for realizing the sustainability benefits of autonomous urban mobility.
